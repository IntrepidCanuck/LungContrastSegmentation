{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Your name and UNI","metadata":{"_kg_hide-output":true}},{"cell_type":"markdown","source":"# Deep Learning In Biomedical Imaging\n## BMENE4460-2022-1\n\n### <span style=\"color:red\">Midterm examination</span> - Vanilla U-Net for single cell segmentation\n\n#### Due date/time: <span style=\"color:red\">March 22, 12:00 pm noon (EST)</span>\n\n#### Instructions:\n1. You may use any publicly available resources to answer the questions, but you need to ***cite*** them properly to prevent plagiarism.\n2. Using or copying other students' solutions is considered cheating, and you'll be graded **\"0\" for the entire exam**.\n3. We have marked the places that you need to fill according to what has been asked in the PDF.\n4. Please define your variables with ***short and meaningful names***.\n5. Please make sure ***internet access is granted*** on the settings panel.\n6. For this exam, you ***need GPU access***. Please set the \"Accelerator\" as \"GPU\" on the \"Setting\" panel.\n\n#### <span style=\"color:red\">Midterm Questions:</span> Please find the questions in the cell blocks below and implement them by yourself. Please note that you can not use any predefined functions from any libraries for Q5 (calculating JI):\nOverall points: 100\n1. Converting color image to grayscale images (20 pts)\n2. Implementing histogram equalization as a preprocessing procedure (20 pts)\n3. Modifying the UNet architecture by adding another encoding and decoding layers (20 pts)\n4. Printing the number of total parameters of your model (20 pts)\n5. Implementing Jaccard index as an evaluation metrics (20 pts)\n6. (Bonus question, 10 points, it worth 2.5 points in your final grade) Improving the model performance you implemented above. You may find more details at the end of this notebook.\n\n#### How to submit:\nKaggle automatically saves the notebook after few seconds, and you may close the notebook and come back later to complete it. However, we need you to do the following steps to ensure your answers are visible to us for grading purposes. Thus, after you finished your answers, please:\n1. Rename the notebook as \"DLInBMI_MidtermExam_{YourUNI}\" (Replace \"{YourUNI}\" with your UNI!).\n2. Click on the \"Save Version\" button on the top-right side of the window.\n3. On the save popup, save your notebook with Save & Run All (Commit)\" option.\n4. Click on the \"Save\" button and let the Kaggle saves your notebook.\n5. Then click on the \"Share\" button.\n6. On the share popup, change \"Private\" to \"Public\".\n7. Copy the \"Public url\", and click on the \"Save\" button.\n8. Submit your \"Public url\" address on the [CourseWorks](https://courseworks2.columbia.edu/).\n\n**Note:**\n1. You may save multiple versions of your notebook, but the latest version is considered as your final answer.\n2. If you wish us to grade your answers based on a different version of your notebook other than the latest version, please duplicate this template, copy your answers from your desired version, and follow the same procedure to submit. Again, make sure the name of the notebook remains as requested format.\n3. Do not change or save the notebook after the due date/time as it will be considered a late submission and causes a decrease in your final grade.","metadata":{}},{"cell_type":"markdown","source":"#### This is basically the notebook we had provided for lecture 8. However, there are some modification/optimization that you need to consider to implement your part properly. Please read the notebook carefully!\n\n#### Sections:\n1. Add the \"Single Cell Segmentation Dataset\" and create Data Loader\n2. Implement U-Net model (Modified version)\n3. Train the model\n4. Display results","metadata":{}},{"cell_type":"markdown","source":"### Import the necessary packages.","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:22:50.04584Z","iopub.execute_input":"2022-04-17T22:22:50.046214Z","iopub.status.idle":"2022-04-17T22:22:52.574255Z","shell.execute_reply.started":"2022-04-17T22:22:50.046115Z","shell.execute_reply":"2022-04-17T22:22:52.573516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check GPU availability (should be False if you run without GPU Accelerator, to turn it on, go to Settings->Accelerator on the right panel and select GPU).","metadata":{}},{"cell_type":"code","source":"cuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:22:59.437863Z","iopub.execute_input":"2022-04-17T22:22:59.438339Z","iopub.status.idle":"2022-04-17T22:22:59.493383Z","shell.execute_reply.started":"2022-04-17T22:22:59.438297Z","shell.execute_reply":"2022-04-17T22:22:59.492583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also make sure the internet access is on.","metadata":{}},{"cell_type":"markdown","source":"#### Set the random seed for reproducibility.\n\nThe random seed helps to make sure that the model parameter initialization, sequence of random shuffling, and most other nondeterministic operations are kept the same each time you run this notebook.","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(4460)\nnp.random.seed(4460)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:23:02.651983Z","iopub.execute_input":"2022-04-17T22:23:02.652533Z","iopub.status.idle":"2022-04-17T22:23:02.659074Z","shell.execute_reply.started":"2022-04-17T22:23:02.652495Z","shell.execute_reply":"2022-04-17T22:23:02.658286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show the number of images we have in the training, validation and test sets","metadata":{}},{"cell_type":"code","source":"!ls \"/kaggle/input/finaldata/pngProjectData/input folder/Test\" | wc -l","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:23:12.604875Z","iopub.execute_input":"2022-04-17T22:23:12.605152Z","iopub.status.idle":"2022-04-17T22:23:13.307984Z","shell.execute_reply.started":"2022-04-17T22:23:12.605122Z","shell.execute_reply":"2022-04-17T22:23:13.307103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls \"/kaggle/input/finaldata/pngProjectData/input folder/Train\" | wc -l","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:23:16.375371Z","iopub.execute_input":"2022-04-17T22:23:16.375935Z","iopub.status.idle":"2022-04-17T22:23:17.063526Z","shell.execute_reply.started":"2022-04-17T22:23:16.375893Z","shell.execute_reply":"2022-04-17T22:23:17.062596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls \"/kaggle/input/finaldata/pngProjectData/input folder/Valid\" | wc -l","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:23:20.346154Z","iopub.execute_input":"2022-04-17T22:23:20.346447Z","iopub.status.idle":"2022-04-17T22:23:21.055624Z","shell.execute_reply.started":"2022-04-17T22:23:20.346414Z","shell.execute_reply":"2022-04-17T22:23:21.054784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3 Create the DataLoaders for each set (train, valid, and test)\n\nUnlike the MNIST example, we cannot load all the images and their masks at once and train our network. We need to read each batch of the images/masks and then train the network with that batch. This process should be repeated for other batches. This way, we will only have one batch at a time on the memory, preventing memory overflow.\n\nTo do so, we need a `BasicDataset` instance that reads an image and respective segmentation mask. \\\nIn below, we implemented the `BasicDataset` class that does the job. Furthermore, you can implement any kinds of preprocessing you wish to do in this implementation. For example, in this class, we resized the resolutions to $256\\times256$ pixels to speed up the training process (Original resolution is $512\\times512$) and reduce memory consumption. In addition, we scaled the intensities from $[0, 255]$ to $[0, 1]$. \\\nSince we will use **Pytorch** and Pytroch's input shape format is $[Batch, Channels, Height, Width]$ rather than $[Batch, Height, Width, Channels]$, we need to changed the image axis as well.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:red\">Q1&2: Please answer Q1 and Q2 below:</span>","metadata":{}},{"cell_type":"code","source":"\"\"\"filepath = \"/kaggle/input/dlbmidataset/pngProjectData/input folder/Test\"\nfilenames = os.listdir(filepath)\n\nnumSlices = 18\nimageDim = 201\nnumTest = 9\nnumValid = 0\nnumTrain = 47\n\nTestData = np.empty(numTest)\n\n# Loop through each of the filenames\nfor ind, f in enumerate(filenames):\n    img_path = os.path.join(filepath, f)\n    \n    # Initialize array for all 18 channels\n    imageData = np.zeros((imageDim,imageDim,numSlices))\n    \n    # Loop through each of the 18 slices\n    for i in range(numSlices):\n        img_name = os.path.join(img_path, str(i)+'.png')\n        \n        # Open the image\n        image = Image.open(img_name)\n        \n        # Convert image to numpy array\n        imageData[:,:,i] = np.asarray(image)\n        \n    # Store it with all the other data\n    TestData[ind] = imageData\"\"\"\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T16:53:57.469572Z","iopub.execute_input":"2022-04-16T16:53:57.469882Z","iopub.status.idle":"2022-04-16T16:53:57.532573Z","shell.execute_reply.started":"2022-04-16T16:53:57.46984Z","shell.execute_reply":"2022-04-16T16:53:57.531642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_root_folder = '../input/finaldata/pngProjectData/input folder/'\ntruth_root_folder = '../input/finaldata/pngProjectData/ground truth folder/'\nnumSlices = 18\nactualImgSize = 201\nimageDim = 208\nnumTest = 9\nnumValid = 0\nnumTrain = 46\n\nclass BasicDataset(TensorDataset):\n    # This function takes folder name ('train', 'valid', 'test') as input and creates an instance of BasicDataset according to that folder.\n    # Also if you'd like to have less number of samples (for evaluation purposes), you may set the `n_sample` with an integer.\n    def __init__(self, folder, n_sample=None):\n        self.imgs_dir = os.path.join(data_root_folder, folder)\n        self.masks_dir = os.path.join(truth_root_folder, folder)\n        #self.imgs_dir = os.path.join(self.folder, 'image')\n        #self.masks_dir = os.path.join(self.folder, 'mask')\n        \n        self.img_dirs = sorted(glob.glob(os.path.join(self.imgs_dir, '*.gz')))\n        self.mask_dirs = sorted(glob.glob(os.path.join(self.masks_dir, '*.gz')))\n        \n        assert len(self.img_dirs) == len(self.mask_dirs), 'There are some missing images or masks in {0}'.format(folder)\n        \n        # If n_sample is not None (It has been set by the user)\n        if not n_sample or n_sample > len(self.img_dirs):\n            n_sample = len(self.img_dirs)\n        \n        self.n_sample = n_sample\n        self.ids = list([i+1 for i in range(n_sample)])\n            \n    # This function returns the lenght of the dataset (AKA number of samples in that set)\n    def __len__(self):\n        return self.n_sample\n    \n    \n    # This function takes an index (i) which is between 0 to `len(BasicDataset)` (The return of the previous function), then returns RGB image, \n    # mask (Binary), and the index of the file name (Which we will use for visualization). The preprocessing step is also implemented in this function.\n    def __getitem__(self, i):\n        idx = self.ids[i]\n        fileName = self.img_dirs[i]\n        maskFileName = self.mask_dirs[i]\n        \n        ### RUN THIS SECTION FOR ALL 18 CHANNELS AT ONCE ------------------------------------\n        # Initialize array for all 18 channels\n        imageData = np.zeros((imageDim,imageDim,numSlices))\n        maskData = np.zeros((imageDim,imageDim,numSlices))\n        \n        for i in range(numSlices):\n            #img = cv2.imread(os.path.join(fileName, str(i)+'.png'), cv2.IMREAD_COLOR)\n            img = cv2.imread(os.path.join(fileName, str(i)+'.png'),cv2.IMREAD_GRAYSCALE)\n            mask = cv2.imread(os.path.join(maskFileName, str(i)+'.png'),cv2.IMREAD_GRAYSCALE)\n            \n            try:\n                #img = cv2.resize(img, (imageDim,imageDim))\n                #mask = cv2.resize(mask, (imageDim,imageDim))\n                img = cv2.resize(img, (imageDim,imageDim),interpolation = cv2.INTER_AREA)\n                mask = cv2.resize(mask, (imageDim,imageDim),interpolation = cv2.INTER_AREA)\n            \n            except Exception as e:\n                print(str(e))\n                print(str(os.path.join(fileName, str(i)+'.png')))\n            \n            # Put the image slice into the data array\n            imageData[:,:,i] = np.asarray(img) \n            maskData[:,:,i] = np.asarray(mask) \n           \n        # HWC to CHW\n        imageData = np.transpose(imageData, (2, 0, 1))\n        maskData = np.transpose(maskData, (2, 0, 1))\n        \n        ### RUN THIS SECTION TO ONLY LOOK AT ONE SLICE PER PATIENT---------------------------\n        \n        '''# Initialize arrays for one slice from each patient\n        imageData = np.zeros((imageDim,imageDim,1))\n        maskData = np.zeros((imageDim,imageDim,1))\n        \n        # Choose which slice we want to use\n        i = 6\n        \n        img = cv2.imread(os.path.join(fileName, str(i)+'.png'),cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(os.path.join(maskFileName, str(i)+'.png'),cv2.IMREAD_GRAYSCALE)\n        \n        # Resize the images\n        img = cv2.resize(img, (imageDim,imageDim),interpolation = cv2.INTER_AREA)\n        mask = cv2.resize(mask, (imageDim,imageDim),interpolation = cv2.INTER_AREA)\n        \n        # Put the image in\n        imageData[:,:,0] = np.asarray(img)\n        maskData[:,:,0] = np.asarray(mask)\n        \n        # HWC to CHW\n        imageData = np.transpose(imageData, (2, 0, 1))\n        maskData = np.transpose(maskData, (2, 0, 1))'''\n        \n        ### --------------------------------------------------------------------------------\n        \n        # Scale between 0 to 1\n        imageData = imageData / 255.0\n        maskData = maskData / 255.0\n\n        \n        # Make sure that the mask are binary (0 or 1)\n        #mask[mask <= 0.5] = 0\n        #mask[mask > 0.5] = 1\n        \n        # Add an axis to the image array so that it is in [channel, height, width] format.\n        #img = np.expand_dims(img, axis=0)\n               \n        \n\n        return {\n            'image': torch.from_numpy(imageData).type(torch.FloatTensor),\n            'mask': torch.from_numpy(maskData).type(torch.FloatTensor),\n            'img_id': idx\n        }","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:24:14.807709Z","iopub.execute_input":"2022-04-17T22:24:14.808229Z","iopub.status.idle":"2022-04-17T22:24:14.82614Z","shell.execute_reply.started":"2022-04-17T22:24:14.808188Z","shell.execute_reply":"2022-04-17T22:24:14.825441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can create our three datasets and display the number of samples in each set in a pythonic way.","metadata":{}},{"cell_type":"code","source":"# Create train, validation, and test dataset instances\ntrain_dataset = BasicDataset('Train')\nvalid_dataset = BasicDataset('Valid')\ntest_dataset = BasicDataset('Test')\n\nplt.figure(figsize=(12,8))\nplt.title('Data split distribution')\nplt.bar(0, len(train_dataset), label='Train')\nplt.bar(1, len(valid_dataset), label='Validation')\nplt.bar(2, len(test_dataset), label='Test')\nplt.ylabel('Number of samples')\nplt.xticks([0,1,2],['Train', 'Validation', 'Test'])\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:24:22.080817Z","iopub.execute_input":"2022-04-17T22:24:22.081772Z","iopub.status.idle":"2022-04-17T22:24:22.27764Z","shell.execute_reply.started":"2022-04-17T22:24:22.08171Z","shell.execute_reply":"2022-04-17T22:24:22.276967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, let's check if our `BasicDataset` implementation works by pulling out a random sample of the training set.\n\n(Don't forget that we need to reverse some of the preprocessing steps, like changing the axis format and rescaling the image intensity to `[0, 255]`)","metadata":{}},{"cell_type":"code","source":"sample = np.random.randint(0, len(train_dataset))\ndata = train_dataset.__getitem__(sample)\nx = data['image']\ny = data['mask']\nidx = data['img_id']\n\nprint(f'x shape is {x.shape}')\nprint(f'y shape is {y.shape}')\n\nplt.figure(figsize=(12, 8), dpi=100)\nplt.suptitle(f'Sample {idx:04d}')\n\n## USE THIS SECTION IF USING ALL 18 CHANNELS\nimg = x[3,:,:]\nmask = y[3,:,:]\n\n## USE THIS SECTION IF USING ONLY ONE CHANNEL PER PATIENT\n#img = x[0,:,:]\n#mask = y[0,:,:]\n\n\nplt.subplot(1, 2, 1)\nplt.title('Image')\nplt.imshow(img, cmap='gray')\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.title('Mask')\nplt.imshow(mask, cmap='gray')\nplt.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:24:33.823232Z","iopub.execute_input":"2022-04-17T22:24:33.823554Z","iopub.status.idle":"2022-04-17T22:24:34.424775Z","shell.execute_reply.started":"2022-04-17T22:24:33.823522Z","shell.execute_reply":"2022-04-17T22:24:34.424056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have made sure our dataset implementation works fine, we can make the `DataLoader` per each set. We will set the batch size as **4** for all sets.\n\n<span style=\"color:red;font-size:18px;\" font>Important Note</span>: For this tutorial, we choose to work with **1000** samples for training, **200** samples for validation, and **200** samples for testing to reduce the training time. In your final project, you should use the whole dataset.","metadata":{}},{"cell_type":"code","source":"# Re-create train, validation, and test dataset instances to reduce the number of samples and expedite the training process.\ntrain_dataset = BasicDataset('Train', n_sample=numTrain)\nvalid_dataset = BasicDataset('Valid', n_sample=numValid)\ntest_dataset = BasicDataset('Test', n_sample=numTest)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=2, pin_memory=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=3, num_workers=2, pin_memory=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=3, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:24:43.067336Z","iopub.execute_input":"2022-04-17T22:24:43.067619Z","iopub.status.idle":"2022-04-17T22:24:43.080225Z","shell.execute_reply.started":"2022-04-17T22:24:43.067587Z","shell.execute_reply":"2022-04-17T22:24:43.079286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Implement U-Net model (Modified version)\n\nNow that we have set up our data loaders, we can implement the architecture.\n\n**We provided the code (Class UNet) for the following architecture**:\n\n<div align=\"center\">\n  <img width=\"800px\" src=\"https://github.com/soroush361/AoE_BME/blob/main/modified_UNet_arch_2.png?raw=true\" />\n</div>\n\n\n**You are asked to modifed the provided code to implement the following architecture: \n**\n<div align=\"center\">\n  <img width=\"800px\" src=\"https://raw.githubusercontent.com/XuzheZ/PTNet3D/main/Picture1.png\" />\n</div>\n\n\nTo implement this modified U-Net, we first define four blocks that we will use multiple times while designing the complete architecture. \\\n`DoubleCov` is block that contains these layers: Conv2d->BatchNormalization->ReLU->Conv2d->BatchNormalization->ReLU \\\n`Down` is a downsampling block that contains Maxpooling and `DoubleConv` after Maxpooling. (Decoding layers) \\\n`Up` is an upsampling block that upsamples the input then pass it through a `DoubleConv.` (Encoding Layers) \\\n`OutConv` is just a $(1\\times1)$ 2D convolution followed by a Sigmoid activation that serves as an output layer in our U-Net model.\n","metadata":{}},{"cell_type":"code","source":"######################################## Double Convolution\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n    \n######################################## Maxpooling followed by Double Convolution\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\n######################################## Upsampling followed by Double Convolution\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up_conv = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n        ) \n        self.conv = DoubleConv(out_channels * 2, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up_conv(x1)\n        x = torch.cat([x1, x2], dim=1)\n        x = self.conv(x)\n        return x\n\n######################################## Output layer (1x1 Convolution followed by SoftMax activation)\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv_sigmoid = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.conv_sigmoid(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:24:47.933971Z","iopub.execute_input":"2022-04-17T22:24:47.934256Z","iopub.status.idle":"2022-04-17T22:24:47.950836Z","shell.execute_reply.started":"2022-04-17T22:24:47.934224Z","shell.execute_reply":"2022-04-17T22:24:47.949804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can mix the above modules to build our architecture.\n\nWe will assign `name`, `n_channels`, and `n_classes` to our implementation for organization purposes. In this case, we may set `name` as an arbitrary name (e.g., 'MyUNet'), `n_channels` must be **1** (Since the input image is gray), and `n_classes` must be **2** (One channel to show the likelihood for background and one for the cells).","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:red\">Q3. Modify the UNet code based on the figure above.</span>\n**Please NOTICE that the code provided below is not what we ask for. We ask you to modify the code below to match the *second network architecture***","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    \n    \n    \n    def __init__(self, name, n_channels, n_classes):\n        super(UNet, self).__init__()\n        self.name = name\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        '''[(Q3) Adding additional encoding and decoding layers below]'''\n        self.inputL = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        self.down4 = Down(512, 1024)\n        \n        self.up1 = Up(1024, 512)\n        self.up2 = Up(512, 256)\n        self.up3 = Up(256, 128)\n        self.up4 = Up(128, 64)\n        self.outputL = OutConv(64, n_classes)\n        \n        '''[(Q3) Modify the forward module below]'''\n    def forward(self, x):\n        x1 = self.inputL(x)\n        \n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        b = self.down4(x4)\n        \n        x = self.up1(b, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        \n        x = self.outputL(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:24:51.791209Z","iopub.execute_input":"2022-04-17T22:24:51.792014Z","iopub.status.idle":"2022-04-17T22:24:51.802229Z","shell.execute_reply.started":"2022-04-17T22:24:51.791961Z","shell.execute_reply":"2022-04-17T22:24:51.801491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can create an instance of our implemented model.","metadata":{}},{"cell_type":"code","source":"## USE THIS IF USING ALL 18 CHANNELS\n# define the channel number first\ninp_channel = 18\nopt_channel = 18\n\n\n## USE THIS IF ONLY USING ONE CHANNEL PER PATIENT\n#inp_channel = 1\n#opt_channel = 2\n\n\nmy_UNet = UNet('MyUNet', inp_channel, opt_channel)\nmy_UNet.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:25:01.751724Z","iopub.execute_input":"2022-04-17T22:25:01.752446Z","iopub.status.idle":"2022-04-17T22:25:04.834298Z","shell.execute_reply.started":"2022-04-17T22:25:01.752407Z","shell.execute_reply":"2022-04-17T22:25:04.833618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"color:red\">Q4. Printing the number of total model parameters here</span>","metadata":{}},{"cell_type":"code","source":"#'''[(Q4) Printing the parameter number]'''","metadata":{"execution":{"iopub.status.busy":"2022-03-18T21:18:53.518743Z","iopub.execute_input":"2022-03-18T21:18:53.519188Z","iopub.status.idle":"2022-03-18T21:18:53.522362Z","shell.execute_reply.started":"2022-03-18T21:18:53.519156Z","shell.execute_reply":"2022-03-18T21:18:53.521608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although we have not trained our model, we may see the output with random weights. Thus, we will pull a batch of test data loader, get the network's output, and plot it.","metadata":{}},{"cell_type":"code","source":"\"\"\"# Take the first batch\nfor batch in test_dataloader:\n    sample_batch = batch\n    break\n    \n# Generat network prediction\nwith torch.no_grad():\n    y_pred = my_UNet(sample_batch['image'].cuda())\n\n# Print the shapes of the images, masks, predicted masks\nprint('Sample batch \\'image \\'shape is: {0}\\nSample batch \\'mask\\' shape is: {1}\\nPredicted mask shape is: {2}'.format(sample_batch['image'].shape, \n                                                                                                                       sample_batch['mask'].shape,\n                                                                                                                       y_pred.shape\n                                                                                                                      ))\n\n# Conver Pytorch tensor to numpy array then reverse the preprocessing steps\nimg = (sample_batch['image'][0][0].numpy() * 255).astype('uint8')\nmsk = (sample_batch['mask'][0].numpy() * 255).astype('uint8')\n\n# Using `np.argmax()`, we may choose the maximum likelihood to assign a label for each pixel\npred_msk_binary = (np.argmax(y_pred.cpu().numpy()[0], axis=0) * 255).astype('uint8')\n# pred_msk_binary = ((y_pred.cpu().numpy()[0][0,:,:] > 0.5) * 255).astype('uint8')\n\n# Take the image id for display\nimg_id = sample_batch['img_id'][0]\n\n# Plot the smaple, ground truth, the prediction probability map, and the final predicted mask\nplt.figure(figsize=(24,18))\nplt.suptitle(f'Test sample Image {img_id}', fontsize=18)\n\nplt.subplot(2,3,1)\nplt.title('Input Image', fontsize=15)\nplt.imshow(img, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2,3,2)\nplt.title('Ground Truth', fontsize=15)\nplt.imshow(msk, cmap='gray')\nplt.axis('off')\n\n\nplt.subplot(2,3,3)\nplt.title('Non-trained Binary Prediction', fontsize=15)\nplt.imshow(pred_msk_binary, cmap='gray')\nplt.axis('off')\n\ninput_overlayed_GT = img.copy()\ninput_overlayed_GT = cv2.cvtColor(input_overlayed_GT, cv2.COLOR_GRAY2RGB)\ninput_overlayed_GT[msk == 255, :] = [0, 255, 0]\nplt.subplot(2,3,4)\nplt.title('Input Image overlayed with Ground Truth', fontsize=15)\nplt.imshow(input_overlayed_GT)\nplt.axis('off')\n\ninput_overlayed_Pred = img.copy()\ninput_overlayed_Pred = cv2.cvtColor(input_overlayed_Pred, cv2.COLOR_GRAY2RGB)\ninput_overlayed_Pred[pred_msk_binary == 255, :] = [255, 0, 0]\nplt.subplot(2,3,5)\nplt.title('Input Image overlayed with Prediction', fontsize=15)\nplt.imshow(input_overlayed_Pred)\nplt.axis('off')\n\nGT_overlayed_prediction = np.zeros_like(img)\nGT_overlayed_prediction = cv2.cvtColor(GT_overlayed_prediction, cv2.COLOR_GRAY2RGB)\nGT_overlayed_prediction[msk == 255, 1] = 255\nGT_overlayed_prediction[pred_msk_binary == 255, 0] = 255\nplt.subplot(2,3,6)\nplt.title('Ground Truth overlayed with Prediction', fontsize=15)\nplt.imshow(GT_overlayed_prediction)\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-16T00:15:18.851661Z","iopub.execute_input":"2022-03-16T00:15:18.852405Z","iopub.status.idle":"2022-03-16T00:15:25.661539Z","shell.execute_reply.started":"2022-03-16T00:15:18.852154Z","shell.execute_reply":"2022-03-16T00:15:25.660637Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the predicted mask is not nearly close to the ground truth because we didn't train the network!","metadata":{"execution":{"iopub.status.busy":"2021-10-30T22:12:50.780538Z","iopub.execute_input":"2021-10-30T22:12:50.781017Z","iopub.status.idle":"2021-10-30T22:12:50.786107Z","shell.execute_reply.started":"2021-10-30T22:12:50.78097Z","shell.execute_reply":"2021-10-30T22:12:50.785164Z"}}},{"cell_type":"markdown","source":"### 3. Train the model","metadata":{}},{"cell_type":"markdown","source":"#### 3.1 Optimizer and Loss function","metadata":{}},{"cell_type":"markdown","source":"Before working on the training loop, let's define the **optimizer** and the **loss function**.\nIn this example, we will use the **\"ADAM\"** algorithm for the optimizer, and for the loss function, we will use **\"Cross-Entropy\"**. You may change these later to see how different optimization algorithms and loss functions may affect the performance.\n\nFor the optimizer, we need to provide the network parameters and define the learning rate. Let's set it as `0.001` for now.","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(my_UNet.parameters(), lr=0.001)\n#loss_function = nn.CrossEntropyLoss()\nloss_function = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:25:54.765362Z","iopub.execute_input":"2022-04-17T22:25:54.765665Z","iopub.status.idle":"2022-04-17T22:25:54.772535Z","shell.execute_reply.started":"2022-04-17T22:25:54.765633Z","shell.execute_reply":"2022-04-17T22:25:54.769981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2 Training loop","metadata":{}},{"cell_type":"markdown","source":"In MNIST example, you saw the training loop is very complecated and there are alot of things happening at the same time. Thus, we provide you with a single function, name `traine_net`, that takes the following arguments and trains the network. The input arguments are:\n\n1. `net`: The model you want to train.\n2. `epochs`: Number of epochs you want to train the network.\n3. `train_dataloader`: The training set `DataLoader`.\n4. `valid_dataloader`: The validation set `DataLoader`.\n5. `optimizer`: The optimizer algorithm.\n6. `loss_function`: The loss function.\n\nThis function creates a directory in `/kaggle/working/{model_name}`, and saves the model weights per each epoch (starts at 1 for the first epoch). Also, it will calculate the [DICE score](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) for both training and validation samples.\n\n<span style=\"color:red\">In Question 5, we asked you to add Jaccard function as well and return that index too.</span>\n\nThen, the return values of this function are:\n\n1. `train_loss`: A list of average training loss per each epoch.\n2. `train_dice`: A list of average training DICE score per each epoch.\n3. `train_jaccard`: A list of average training Jaccard score per each epoch.\n4. `valid_loss`: A list of average validation loss per each epoch.\n5. `valid_dice`: A list of average validation DICE score per each epoch.\n6. `valid_jaccard`: A list of average validation Jaccard score per each epoch.","metadata":{"execution":{"iopub.status.busy":"2021-10-30T21:43:51.215176Z","iopub.execute_input":"2021-10-30T21:43:51.215473Z","iopub.status.idle":"2021-10-30T21:43:51.253806Z","shell.execute_reply.started":"2021-10-30T21:43:51.215438Z","shell.execute_reply":"2021-10-30T21:43:51.25287Z"}}},{"cell_type":"markdown","source":"### <span style=\"color:red\">Q5. Implementing Jaccard Index below</span>","metadata":{}},{"cell_type":"code","source":"# Define a function that computes the Jaccard score for binary segmentation\ndef jaccard_index(y_pred, y_true):\n        '''[(Q5) Your answer here - hint: check the equation of Jaccard Index here:https://en.wikipedia.org/wiki/Jaccard_index, do not use predefined jaccard index functions provided in any libraries ]'''\n        eps = 0.0001\n        intersection = torch.dot(y_pred.view(-1).float(), y_true.view(-1).float())\n        union = torch.sum(y_pred) + torch.sum(y_true)\n        jaccard_index = ((intersection.float() + eps)/(union.float() + eps)).cpu().detach().numpy()       \n        return jaccard_index\n\n# Define a function that computes the DICE score for binary segmentation\n#def dice_coeff_binary(y_pred, y_true):\n#        \"\"\"Values must be only zero or one.\"\"\"\n#        eps = 0.0001\n#        inter = torch.dot(y_pred.view(-1).float(), y_true.view(-1).float())\n#        union = torch.sum(y_pred.float()) + torch.sum(y_true.float())\n#        return ((2 * inter.float() + eps) / (union.float() + eps)).cpu().numpy()\n    \n        \n    \n\n# The training function\ndef train_net(net, epochs, train_dataloader, valid_dataloader, optimizer, loss_function):\n    \n    if not os.path.isdir('{0}'.format(net.name)):\n        os.mkdir('{0}'.format(net.name))\n    \n    n_train = len(train_dataloader)\n    n_valid = len(valid_dataloader)    \n    \n    train_loss = list()\n    valid_loss = list()\n    #train_dice = list()\n    #valid_dice = list()\n    train_jaccard = list()\n    valid_jaccard = list()\n    \n    for epoch in range(epochs):\n        \n        ################################################################################################################################\n        ########################################################### Training ###########################################################\n        ################################################################################################################################\n        net.train()\n        train_batch_loss = list()\n        #train_batch_dice = list()\n        train_batch_jaccard = list()\n        \n        for i, batch in enumerate(train_dataloader):\n\n            # Load a batch and pass it to the GPU\n            imgs = batch['image'].cuda()\n            true_masks = batch['mask'].cuda()\n\n            # Produce the estimated mask using current weights\n            y_pred = net(imgs)\n\n            # Compute the loss for this batch and append it to the epoch loss\n            loss = loss_function(y_pred, true_masks)\n            batch_loss = loss.item()\n            train_batch_loss.append(batch_loss)\n\n            # Make the binary mask to compute the DICE score. Since the y_pred is a Pytoch tensor, we use `torch.argmax()` instead of `np.argmax()`.\n            # the axis must be 1 instead of 0 because the format is [batch, channel, height, width]\n            pred_binary = torch.argmax(y_pred, axis=1)\n            \n            # Compute the DICE score for this batch and append it to the epoch dice\n            #batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n            #train_batch_dice.append(batch_dice_score)\n            \n            # Compute the Jaccard score here and \n            #batch_jaccard_score = jaccard_index(pred_binary, true_masks)\n            batch_jaccard_score = jaccard_index(y_pred, true_masks)\n            train_batch_jaccard.append(batch_jaccard_score)\n            \n\n            # Reset gradient values\n            optimizer.zero_grad()\n\n            # Compute the backward losses\n            loss.backward()\n\n            # Update the weights\n            optimizer.step()\n            \n            # Print the progress\n            print(f'EPOCH {epoch + 1}/{epochs} - Training Batch {i+1}/{n_train} - Loss: {batch_loss}, Jaccard score: {batch_jaccard_score}            ', end='\\r')\n        \n        average_training_loss = np.array(train_batch_loss).mean()\n        #average_training_dice = np.array(train_batch_dice).mean()\n        average_training_jaccard = np.array(train_batch_jaccard).mean()\n        train_loss.append(average_training_loss)\n        #train_dice.append(average_training_dice)\n        train_jaccard.append(average_training_jaccard)\n        \n        ################################################################################################################################\n        ########################################################## Validation ##########################################################\n        ################################################################################################################################\n        \n        net.eval()\n        valid_batch_loss = list()\n        #valid_batch_dice = list()\n        valid_batch_jaccard = list()\n        \n        # This part is almost the same as training with the difference that we will set all layers to evaluation mode (effects some layers such as BN and Dropout) and also\n        # we don't need to calculate the gradient since we are only evaluating current state of the model. This will speed up the process and cause it to consume less memory.\n        with torch.no_grad():\n            for i, batch in enumerate(valid_dataloader):\n\n                # Load a batch and pass it to the GPU\n                imgs = batch['image'].cuda()\n                true_masks = batch['mask'].cuda()\n\n                # Produce the estimated mask using current weights\n                y_pred = net(imgs)\n\n                # Compute the loss for this batch and append it to the epoch loss\n                loss = loss_function(y_pred, true_masks)\n                batch_loss = loss.item()\n                valid_batch_loss.append(batch_loss)\n\n                # Make the binary mask to compute the DICE score. Since the y_pred is a Pytoch tensor, we use `torch.argmax()` instead of `np.argmax()`.\n                # the axis must be 1 instead of 0 because the format is [batch, channel, height, width]\n                pred_binary = torch.argmax(y_pred, axis=1)\n\n                # Compute the DICE score for this batch and append it to the epoch dice\n                #batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n                #valid_batch_dice.append(batch_dice_score)\n                \n                # Compute the Jaccard score here and append the score to the list\n                #batch_jaccard_score = jaccard_index(pred_binary, true_masks)\n                batch_jaccard_score = jaccard_index(y_pred, true_masks)\n                valid_batch_jaccard.append(batch_jaccard_score)\n\n                # Print the progress\n                print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {batch_loss}, Jaccard score: {batch_jaccard_score}            ', end='\\r')\n                \n        average_validation_loss = np.array(valid_batch_loss).mean()\n        #average_validation_dice = np.array(valid_batch_dice).mean()\n        average_validation_jaccard = np.array(valid_batch_jaccard).mean()\n        valid_loss.append(average_validation_loss)\n        #valid_dice.append(average_validation_dice)\n        valid_jaccard.append(average_validation_jaccard)\n        \n        print(f'EPOCH {epoch + 1}/{epochs} - Training Loss: {average_training_loss}, Training Jaccard score: {average_training_jaccard}, Validation Loss: {average_validation_loss}, Validation Jaccard score: {average_validation_jaccard}')\n\n        ################################################################################################################################\n        ###################################################### Saveing Checkpoints #####################################################\n        ################################################################################################################################\n        torch.save(net.state_dict(), f'{net.name}/epoch_{epoch+1:03}.pth')\n    \n    return train_loss, train_jaccard, valid_loss, valid_jaccard","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:26:37.095519Z","iopub.execute_input":"2022-04-17T22:26:37.095842Z","iopub.status.idle":"2022-04-17T22:26:37.121017Z","shell.execute_reply.started":"2022-04-17T22:26:37.095807Z","shell.execute_reply":"2022-04-17T22:26:37.120241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have everything in place, we can call the `train_net` function on our model and let it be trained for **10** epochs.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 20\ntrain_loss, train_jaccard, valid_loss, valid_jaccard = train_net(my_UNet, EPOCHS, train_dataloader, valid_dataloader, optimizer, loss_function)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:26:42.785622Z","iopub.execute_input":"2022-04-17T22:26:42.78635Z","iopub.status.idle":"2022-04-17T22:28:00.399727Z","shell.execute_reply.started":"2022-04-17T22:26:42.786313Z","shell.execute_reply":"2022-04-17T22:28:00.398728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Display results","metadata":{}},{"cell_type":"markdown","source":"Now that we have trained our network, let us see how it will perform on the test set. \n\nFirst, let us plot the learning curve and the DICE scores per epoch.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.suptitle('Learning Curve', fontsize=18)\n\nplt.subplot(1,2,1)\nplt.plot(np.arange(EPOCHS)+1, train_loss, '-o', label='Training Loss')\nplt.plot(np.arange(EPOCHS)+1, valid_loss, '-o', label='Validation Loss')\nplt.xticks(np.arange(EPOCHS)+1)\nplt.xlabel('Epoch', fontsize=15)\nplt.ylabel('Loss', fontsize=15)\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(np.arange(EPOCHS)+1, train_jaccard, '-o', label='Training Jaccard score')\nplt.plot(np.arange(EPOCHS)+1, valid_jaccard, '-o', label='Validation Jaccard score')\nplt.xticks(np.arange(EPOCHS)+1)\nplt.xlabel('Epoch', fontsize=15)\nplt.ylabel('Jaccard score', fontsize=15)\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:33:06.981501Z","iopub.execute_input":"2022-04-17T22:33:06.981809Z","iopub.status.idle":"2022-04-17T22:33:07.478112Z","shell.execute_reply.started":"2022-04-17T22:33:06.981773Z","shell.execute_reply":"2022-04-17T22:33:07.477402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, let us see in which epoch the model obtained the highest validation DICE score and load the weights of that epoch as our best model weights.","metadata":{}},{"cell_type":"code","source":"best_epoch = np.argmax(valid_jaccard) + 1 # The plus one is because the epochs starts at 1.\n\nprint(f'Best epoch is epoch{best_epoch}')\n\nstate_dict = torch.load(f'./MyUNet/epoch_{best_epoch:03}.pth')\n\nmy_UNet.load_state_dict(state_dict)\nmy_UNet.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:33:16.380248Z","iopub.execute_input":"2022-04-17T22:33:16.380502Z","iopub.status.idle":"2022-04-17T22:33:16.48615Z","shell.execute_reply.started":"2022-04-17T22:33:16.380473Z","shell.execute_reply":"2022-04-17T22:33:16.485407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can check the performance of the best model on our previously pulled example. (The same scripts must work!)","metadata":{}},{"cell_type":"code","source":"# Take the first batch of test set\nfor batch in test_dataloader:\n    sample_batch = batch\n    break\n    \n# Generat network prediction\nwith torch.no_grad():\n    y_pred = my_UNet(sample_batch['image'].cuda())\n\n# Print the shapes of the images, masks, predicted masks\nprint('Sample batch \\'image \\'shape is: {0}\\nSample batch \\'mask\\' shape is: {1}\\nPredicted mask shape is: {2}'.format(sample_batch['image'].shape, \n                                                                                                                       sample_batch['mask'].shape,\n                                                                                                                       y_pred.shape\n                                                                                                                      ))\n\n# Convert Pytorch tensor to numpy array then reverse the preprocessing steps\nimg = (sample_batch['image'][0].numpy() * 255).astype('uint8')\nmsk = (sample_batch['mask'][0].numpy() * 255).astype('uint8')\n\n# Using `np.argmax()`, we may choose the maximum likelihood to assign a label for each pixel\n#pred_msk_binary = (np.argmax(y_pred.cpu().numpy()[0], axis=0) * 255).astype('uint8')\npred_msk_binary = ((y_pred.cpu().numpy()[0][8,:,:] > 0.5) * 255).astype('uint8')\n\n# Take the image id for display\nimg_id = sample_batch['img_id'][0]\n\n# Plot the smaple, ground truth, the prediction probability map, and the final predicted mask\nplt.figure(figsize=(24,18))\nplt.suptitle(f'Test sample Image {img_id}', fontsize=18)\n\nplt.subplot(2,3,1)\nplt.title('Input Image', fontsize=15)\nplt.imshow(img[0,:,:], cmap='gray')\nplt.axis('off')\n\nplt.subplot(2,3,2)\nplt.title('Ground Truth', fontsize=15)\nplt.imshow(msk[0,:,:], cmap='gray')\nplt.axis('off')\n\n\nplt.subplot(2,3,3)\nplt.title('Final Binary Prediction', fontsize=15)\nplt.imshow(pred_msk_binary, cmap='gray')\nplt.axis('off')\n\ninput_overlayed_GT = img.copy()\ninput_overlayed_GT = cv2.cvtColor(input_overlayed_GT, cv2.COLOR_GRAY2RGB)\ninput_overlayed_GT[msk == 255, :] = [0, 255, 0]\nplt.subplot(2,3,4)\nplt.title('Input Image overlayed with Ground Truth', fontsize=15)\nplt.imshow(input_overlayed_GT[7,:,:])\nplt.axis('off')\n\ninput_overlayed_Pred = img.copy()\ninput_overlayed_Pred = cv2.cvtColor(input_overlayed_Pred, cv2.COLOR_GRAY2RGB)\ninput_overlayed_Pred[pred_msk_binary == 255, :] = [255, 0, 0]\nplt.subplot(2,3,5)\nplt.title('Input Image overlayed with Prediction', fontsize=15)\nplt.imshow(input_overlayed_Pred[7,:,:])\nplt.axis('off')\n\nGT_overlayed_prediction = np.zeros_like(img)\nGT_overlayed_prediction = cv2.cvtColor(GT_overlayed_prediction, cv2.COLOR_GRAY2RGB)\nGT_overlayed_prediction[msk == 255, 1] = 255\nGT_overlayed_prediction[pred_msk_binary == 255, 0] = 255\nplt.subplot(2,3,6)\nplt.title('Ground Truth overlayed with Prediction', fontsize=15)\nplt.imshow(GT_overlayed_prediction[7,:,:])\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:34:43.392398Z","iopub.execute_input":"2022-04-17T22:34:43.392666Z","iopub.status.idle":"2022-04-17T22:34:44.466432Z","shell.execute_reply.started":"2022-04-17T22:34:43.392636Z","shell.execute_reply":"2022-04-17T22:34:44.464903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it is clear, the model is predicting the ground truth much better compared to before training.\n\nHowever, we need to compute the overall performance of the model on all test samples. Then, similar to the `train_net` function, we can define a `test_net` function that tests our model on test samples and save the prediction masks in the `/kaggle/working/pred_mask` folder.\n\nThis function takes the following arguments:\n\n1. `net`: The model we want to test.\n2. `test_dataloader`: The `DataLoader` for the test set.\n3. `loss_function`: The loss function to calculate the loss.\n\nThis function returns:\n1. `test_loss`: The average test loss.\n2. `test_dice`: The average test DICE score.\n3. `test_jaccard`: The average test Jaccard score.\n4. `test_accuracy`: The overall accuracy of the model.\n5. `test_CM`: The normalized confusion matrix of the model.\n\n","metadata":{}},{"cell_type":"code","source":"def test_net(net, test_dataloader, loss_function):\n    # Create the pred_mask folder\n    if not os.path.isdir('/kaggle/working/pred_mask'):\n        os.mkdir('/kaggle/working/pred_mask')\n    \n    net.eval()\n    \n    n_test = len(test_dataloader)\n    test_batch_loss = list()\n    test_batch_dice = list()\n    test_batch_jaccard = list()\n    test_batch_accuray = list()\n    test_batch_CM = list()\n\n    # This part is almost the same as the validation loop in `train_net` function. \n    # The difference is that we will calculate the accuracy and confusion matrix per each batch and save the predicted images.\n    with torch.no_grad():\n        for i, batch in enumerate(test_dataloader):\n\n            # Load a batch and pass it to the GPU\n            imgs = batch['image'].cuda()\n            true_masks = batch['mask'].cuda()\n            img_ids = batch['img_id'].numpy().astype('int')\n\n            # Produce the estimated mask using current weights\n            y_pred = net(imgs)\n\n            # Compute the loss for this batch and append it to the epoch loss\n            loss = loss_function(y_pred, true_masks)\n            batch_loss = loss.item()\n            test_batch_loss.append(batch_loss)\n\n            # Make the binary mask to compute the DICE score. Since the y_pred is a Pytoch tensor, we use `torch.argmax()` instead of `np.argmax()`.\n            # the axis must be 1 instead of 0 because the format is [batch, channel, height, width]\n            pred_binary = torch.argmax(y_pred, axis=1)\n\n            # Compute the DICE score for this batch and append it to the epoch dice\n            batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n            test_batch_dice.append(batch_dice_score)\n            \n            # Compute the Jaccard score for this batch and append it to the epoch dice\n            batch_jaccard_score = jaccard_coeff_binary(pred_binary, true_masks)\n            test_batch_jaccard.append(batch_jaccard_score)\n            \n            # Save the predicted masks\n            for idx, pred_msk in enumerate(pred_binary):\n                cv2.imwrite(f'/kaggle/working/pred_mask/pred_mask_{img_ids[idx]:04}.png', pred_msk.cpu().numpy())\n            \n            # Vectorize the true mask and predicted mask for this batch\n            vectorize_true_masks = true_masks.view(-1).cpu().numpy()\n            vectorize_pred_masks = pred_binary.view(-1).cpu().numpy()\n            \n            # Compute the accuracy for this batch and append to the overall list\n            batch_accuracy = accuracy_score(vectorize_true_masks, vectorize_pred_masks)\n            test_batch_accuray.append(batch_accuracy)\n            \n            # Compute the normalized confusion matrix for this batch and append to the overall list\n            batch_CM = confusion_matrix(vectorize_true_masks, vectorize_pred_masks, normalize='true', labels=[0, 1])\n            test_batch_CM.append(batch_CM)\n\n            # Print the progress\n            print(f'Test Batch {i+1}/{n_test} - Loss: {batch_loss}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}, Accuracy: {batch_accuracy}', end='\\r')\n\n    test_loss = np.array(test_batch_loss).mean()\n    test_dice = np.array(test_batch_dice).mean()\n    test_jaccard = np.array(test_batch_jaccard).mean()\n    test_accuracy = np.array(test_batch_accuray).mean()\n    test_CM = np.array(test_batch_CM).mean(axis=0)\n    \n    return test_loss, test_dice, test_jaccard, test_accuracy, test_CM","metadata":{"execution":{"iopub.status.busy":"2022-03-16T00:24:58.870483Z","iopub.execute_input":"2022-03-16T00:24:58.870813Z","iopub.status.idle":"2022-03-16T00:24:58.887351Z","shell.execute_reply.started":"2022-03-16T00:24:58.870775Z","shell.execute_reply":"2022-03-16T00:24:58.886602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us use the function and see how it works!\n\nNote: The accuracy and confusion matrix are computed on the CPU; thus, this function might be slower.","metadata":{}},{"cell_type":"code","source":"test_loss, test_dice, test_jaccard, test_accuracy, test_CM = test_net(my_UNet, test_dataloader, loss_function)\n\nprint(f'Test Loss: {test_loss}, Test DICE score: {test_dice}, Test Jaccard score: {test_jaccard}, Test overall accuracy: {test_accuracy}')","metadata":{"execution":{"iopub.status.busy":"2022-03-16T00:24:59.522659Z","iopub.execute_input":"2022-03-16T00:24:59.523484Z","iopub.status.idle":"2022-03-16T00:25:20.500298Z","shell.execute_reply.started":"2022-03-16T00:24:59.523446Z","shell.execute_reply":"2022-03-16T00:25:20.499427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cm = pd.DataFrame(test_CM, index = ['Background', 'Cell'],\n                     columns = ['Background', 'Cell'])\nplt.figure(figsize = (12,10))\nplt.title('Confusion matrix')\nsns.heatmap(df_cm, annot = True, fmt='.2%', annot_kws = {\"size\": 15})\nplt.ylim([0, 2]);\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');","metadata":{"execution":{"iopub.status.busy":"2022-03-16T00:27:46.557394Z","iopub.execute_input":"2022-03-16T00:27:46.557858Z","iopub.status.idle":"2022-03-16T00:27:46.825061Z","shell.execute_reply.started":"2022-03-16T00:27:46.557815Z","shell.execute_reply":"2022-03-16T00:27:46.824345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"color:red\">Q6. Bonus Question (10 pts).</span>","metadata":{}},{"cell_type":"markdown","source":"To improve the performance of the model you implemented above, you may consider implementing one of them or combining several of them, re-use the above code to print out your new results.\nYou should have a <span style=\"color:red\">better</span> result comparing to the previous one:\n1. Data preprocessing\n2. Data augmentation\n3. Network architecture (CNN layer, kernel ...)\n4. Loss function \n5. Optimizer \n...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}